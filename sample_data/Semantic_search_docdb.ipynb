{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "import pymongo\n",
    "import json\n",
    "import httpx\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "OPENAI_EMBEDDING_DIMS = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a connection to your Amazon DocumentDB (MongoDB compatibility) cluster and creating the database\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "\"localhost:27017\",\n",
    "username=\"unicom\",\n",
    "password=\"password1234\",\n",
    "retryWrites=False,\n",
    "tls='true',\n",
    "tlsCAFile=\"/home/jheel/Desktop/swen614/term-project-team1-unifamily/api/global-bundle.pem\",\n",
    "tlsAllowInvalidHostnames=True,\n",
    "directConnection=True\n",
    ")\n",
    "db = client.semanticdemo\n",
    "collection = db.movies\n",
    "collection.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data has been successfully uploaded to DocumentDB\n"
     ]
    }
   ],
   "source": [
    "# Loading the DocumentDB database from the example dataset in csv\n",
    "# Example dataset includes just 50 entries and is adapted from https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file = \"demomovies.csv\" \n",
    "data = pd.read_csv(csv_file)\n",
    "# Convert the DataFrame to a list of dictionaries (one per row)\n",
    "data_dict = data.to_dict(orient=\"records\")\n",
    "for row in data_dict:\n",
    "    row[\"embedding_hf\"] = [0.0] * OPENAI_EMBEDDING_DIMS\n",
    "# Insert the data into the DocumentDB collection\n",
    "collection.insert_many(data_dict)\n",
    "print(\"CSV data has been successfully uploaded to DocumentDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Generating text embeddings and storing it with existing data in Amazon DocumentDB\n",
    "\n",
    "# Defining HuggingFace Token and embedding model\n",
    "hf_token = \"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\"\n",
    "embedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "EMBEDDING_DIMS = 384  # Dimension of the embedding vector for the model used\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "OPENAI_EMBEDDING_DIMS = 1536  # Dimension of the embedding vector for OpenAI's model\n",
    "\n",
    "#Define Generate Embedding Function\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    client2 = httpx.Client()\n",
    "    response = client2.post(\n",
    "        embedding_url,\n",
    "        headers={\"Authorization\": f\"Bearer {hf_token}\"},\n",
    "        json={\"inputs\": text}\n",
    "    )\n",
    "    client2.close()\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "    return response.json()\n",
    "\n",
    "def get_openai_embedding(text):\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=OPENAI_EMBEDDING_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Note: Model can take upto 20 secs to start. So, in case of model error, try again again after 20 seconds.\n",
    "\n",
    "# Fetch all documents that have overview field\n",
    "documents_to_update = list(collection.find({'overview': {\"$exists\": True}}))\n",
    "\n",
    "# Define the batch size for processing\n",
    "batch_size = 10  # You can adjust this based on your requirements\n",
    "\n",
    "# Process documents in batches\n",
    "for i in range(0, len(documents_to_update), batch_size):\n",
    "    batch = documents_to_update[i:i + batch_size]\n",
    "\n",
    "    # Generate embeddings for the current batch and store it alongside existing data as new field\n",
    "    for doc in batch:\n",
    "        doc['embedding_hf'] = get_openai_embedding(doc['overview'])\n",
    "\n",
    "    # Update the batch of documents\n",
    "    bulk_operations = [pymongo.ReplaceOne({'_id': doc['_id']}, doc) for doc in batch]\n",
    "    collection.bulk_write(bulk_operations)\n",
    "\n",
    "print(\"Batch processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_index'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating IVFflat index with dotProduct distance metrics\n",
    "# collection.drop_index(\"my_index\")  # Drop the index if it already exists\n",
    "collection.create_index ([(\"embedding_hf\",\"vector\")], \n",
    "    vectorOptions= {\n",
    "        \"type\": \"hnsw\", #You can choose HNSW index as well. With HNSW, you will have to remove \"lists\" parameter and use \"m\" and \"efConstruction\".\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"dimensions\": OPENAI_EMBEDDING_DIMS,\n",
    "        \"m\": 16,\n",
    "        \"efConstruction\": 200},\n",
    "    name=\"my_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.unicom.posts.drop_index(\"my_index\")  # Drop the index if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id_': {'v': 4, 'key': [('_id', 1)], 'ns': 'unicom.posts'},\n",
       " 'description_vector_hnsw': {'v': 4,\n",
       "  'key': [('description_vector', 'vector')],\n",
       "  'vectorOptions': SON([('type', 'hnsw'), ('dimensions', 1536), ('similarity', 'cosine'), ('m', 16), ('efConstruction', 200)]),\n",
       "  'ns': 'unicom.posts'}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.unicom.posts.index_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id_': {'v': 4, 'key': [('_id', 1)], 'ns': 'semanticdemo.movies'},\n",
       " 'my_index': {'v': 4,\n",
       "  'key': [('embedding_hf', 'vector')],\n",
       "  'vectorOptions': SON([('type', 'hnsw'), ('dimensions', 1536), ('similarity', 'cosine'), ('m', 16), ('efConstruction', 200)]),\n",
       "  'ns': 'semanticdemo.movies'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.movies.index_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining which fields in results to project\n",
    "projection = {\n",
    "\"_id\":0,\n",
    "\"title\": 1, \n",
    "\"overview\": 1}\n",
    "\n",
    "#Defining semantic query function\n",
    "def semantic_search(keyword):\n",
    "    query = {\"vectorSearch\" : {\"vector\" : get_openai_embedding(keyword), \"path\": \"embedding_hf\", \"similarity\": \"dotProduct\", \"k\": 3}}\n",
    "    results = collection.aggregate([{'$search': query},{\"$project\": projection}])\n",
    "    return list(results)\n",
    "\n",
    "#Defining keyword query function\n",
    "def keyword_search(keyword):\n",
    "    results = collection.aggregate([{\"$match\": {\"overview\": {\"$regex\": keyword}}},{\"$project\": projection}])\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Harry Potter and the Deathly Hallows: Part 2',\n",
       "  'overview': \"Harry, Ron and Hermione continue their quest to vanquish the evil Voldemort once and for all. Just as things begin to look hopeless for the young wizards, Harry discovers a trio of magical objects that endow him with powers to rival Voldemort's formidable skills.\"},\n",
       " {'title': 'Beauty and the Beast',\n",
       "  'overview': \"A live-action adaptation of Disney's version of the classic 'Beauty and the Beast' tale of a cursed prince and a beautiful young woman who helps him break the spell.\"},\n",
       " {'title': 'Frozen',\n",
       "  'overview': \"Young princess Anna of Arendelle dreams about finding true love at her sister Elsaâ€™s coronation. Fate takes her on a dangerous journey in an attempt to end the eternal winter that has fallen over the kingdom. She's accompanied by ice delivery man Kristoff, his reindeer Sven, and snowman Olaf. On an adventure where she will find out what friendship, courage, family, and true love really means.\"}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doing semantic query example - search for movies with words \"young magician\"\n",
    "semantic_search(\"young magician\")\n",
    "\n",
    "#You can see that search results are semantically similar. The query results do not have the exact words \"young magician\". However, it still manages to find movies like Harry Potter. Next, you can compare these results with keyword search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing keyword query example 1 - search for movies with keyword \"young magician\"\n",
    "keyword_search(\"young magician\")\n",
    "\n",
    "#No results were returned because exact words \"young magician\" were not found in the overview description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing keyword query example 2 - search for movies with keyword \"young wizard\"\n",
    "keyword_search(\"young wizard\")\n",
    "\n",
    "#One result was returned because exact words \"young wizard\" were found in the overview description."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
